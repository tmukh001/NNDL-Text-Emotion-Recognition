# üß† Neural Networks & Deep Learning: Text Emotion Recognition using BERT + CNN

A deep learning project exploring **emotion recognition from text** using hybrid architectures that combine **transformer-based embeddings (BERT)** with **convolutional neural networks (CNN)**. This project was completed for the *SC4001: Neural Networks and Deep Learning* course at NTU Singapore.

---

## üß© Problem Overview

Text Emotion Recognition (TER) involves predicting the emotional intent of textual input‚Äîan essential challenge in **affective computing** and **natural language processing** (NLP). Emotions can be subtle and context-dependent, requiring models to capture both:

- **Local information** (word-level cues)
- **Global information** (sentence/context-level semantics)

We implemented and compared four models across two datasets:
1. CNN (baseline)
2. Bidirectional LSTM (BiLSTM)
3. Attention-enhanced BiLSTM (AttBiLSTM)
4. **BERT + CNN hybrid model** (our proposed method)

---

## üí° Key Features

- Preprocessing pipelines using tokenization, padding, and GloVe/BERT embeddings
- Use of both **GloVe vectors** and **BERT transformers**
- Attention mechanism for improved contextual focus
- Evaluation across **two real-world datasets**:
  - **Crowdflower**: Twitter-based emotions (imbalanced, multi-class)
  - **Emo2019**: Conversation-style text (cleaner and balanced)

---

## üìä Results Summary

| Model       | Dataset     | Best Macro F1 Score | Notable Insights                                 |
|-------------|-------------|---------------------|--------------------------------------------------|
| CNN         | Emo2019     | Moderate            | Fast training but lacks global context           |
| BiLSTM      | Emo2019     | Improved            | Captures context bidirectionally                 |
| AttBiLSTM   | Emo2019     | Better              | Attention boosts relevant word focus             |
| **BERT+CNN**| Emo2019     | **Best**            | Strong embeddings + CNN yields top performance   |
| BERT+CNN    | Crowdflower | Underperformed      | BERT struggles with noisy, domain-mismatched data|

---

## üîç Why It Matters

This project highlights my ability to:

- **Compare model architectures** for complex NLP tasks
- Use **transformer-based models** for embedding generation
- Apply **attention mechanisms** for interpretability and accuracy
- Handle noisy, real-world data (e.g. tweets) with class imbalance
- Perform structured experimentation and explain performance gaps

It reflects my interest in **language modeling**, **NLP for affective applications**, and building models that bridge theory and usability.

---

## üìÅ Key Files

- `roberta_cnn_em_executed.ipynb` ‚Äì BERT+CNN model on Emo2019  
- `roberta_cnn_crowdflower_executed.ipynb` ‚Äì BERT+CNN model on Crowdflower  

---

## üõ†Ô∏è Technologies Used

- Python  
- PyTorch  
- HuggingFace Transformers  
- Keras (for early models)  
- GloVe embeddings  
- Google Colab (GPU training)

---

## üß™ How to Reproduce

```bash
# Install necessary packages
pip install transformers torch scikit-learn

# Open and run the notebooks:
# - roberta_cnn_em_executed.ipynb
# - roberta_cnn_crowdflower_executed.ipynb
```

---

## üë®‚Äçüíª Authors

- **Tathagato Mukherjee**  
BSc (Hons), Data Science & AI, NTU Singapore  

Collaborators:
- Aryan Sethi  
- Shourya Kuchhal

